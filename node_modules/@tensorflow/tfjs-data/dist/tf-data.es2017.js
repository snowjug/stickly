/**
 * @license
 * Copyright 2024 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
(function (global, factory) {
	typeof exports === 'object' && typeof module !== 'undefined' ? factory(exports, require('@tensorflow/tfjs-core')) :
	typeof define === 'function' && define.amd ? define(['exports', '@tensorflow/tfjs-core'], factory) :
	(global = typeof globalThis !== 'undefined' ? globalThis : global || self, factory(global.tf = global.tf || {}, global.tf));
})(this, (function (exports, tf) { 'use strict';

	function _interopNamespaceDefault(e) {
		var n = Object.create(null);
		if (e) {
			Object.keys(e).forEach(function (k) {
				if (k !== 'default') {
					var d = Object.getOwnPropertyDescriptor(e, k);
					Object.defineProperty(n, k, d.get ? d : {
						enumerable: true,
						get: function () { return e[k]; }
					});
				}
			});
		}
		n.default = e;
		return n;
	}

	var tf__namespace = /*#__PURE__*/_interopNamespaceDefault(tf);

	var commonjsGlobal = typeof globalThis !== 'undefined' ? globalThis : typeof window !== 'undefined' ? window : typeof global !== 'undefined' ? global : typeof self !== 'undefined' ? self : {};

	function getAugmentedNamespace(n) {
	  if (n.__esModule) return n;
	  var f = n.default;
		if (typeof f == "function") {
			var a = function a () {
				if (this instanceof a) {
					var args = [null];
					args.push.apply(args, arguments);
					var Ctor = Function.bind.apply(f, args);
					return new Ctor();
				}
				return f.apply(this, arguments);
			};
			a.prototype = f.prototype;
	  } else a = {};
	  Object.defineProperty(a, '__esModule', {value: true});
		Object.keys(n).forEach(function (k) {
			var d = Object.getOwnPropertyDescriptor(n, k);
			Object.defineProperty(a, k, d.get ? d : {
				enumerable: true,
				get: function () {
					return n[k];
				}
			});
		});
		return a;
	}

	var alea$1 = {exports: {}};

	(function (module) {
		// A port of an algorithm by Johannes Baagøe <baagoe@baagoe.com>, 2010
		// http://baagoe.com/en/RandomMusings/javascript/
		// https://github.com/nquinlan/better-random-numbers-for-javascript-mirror
		// Original work is under MIT license -

		// Copyright (C) 2010 by Johannes Baagøe <baagoe@baagoe.org>
		//
		// Permission is hereby granted, free of charge, to any person obtaining a copy
		// of this software and associated documentation files (the "Software"), to deal
		// in the Software without restriction, including without limitation the rights
		// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
		// copies of the Software, and to permit persons to whom the Software is
		// furnished to do so, subject to the following conditions:
		//
		// The above copyright notice and this permission notice shall be included in
		// all copies or substantial portions of the Software.
		//
		// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
		// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
		// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
		// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
		// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
		// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
		// THE SOFTWARE.



		(function(global, module, define) {

		function Alea(seed) {
		  var me = this, mash = Mash();

		  me.next = function() {
		    var t = 2091639 * me.s0 + me.c * 2.3283064365386963e-10; // 2^-32
		    me.s0 = me.s1;
		    me.s1 = me.s2;
		    return me.s2 = t - (me.c = t | 0);
		  };

		  // Apply the seeding algorithm from Baagoe.
		  me.c = 1;
		  me.s0 = mash(' ');
		  me.s1 = mash(' ');
		  me.s2 = mash(' ');
		  me.s0 -= mash(seed);
		  if (me.s0 < 0) { me.s0 += 1; }
		  me.s1 -= mash(seed);
		  if (me.s1 < 0) { me.s1 += 1; }
		  me.s2 -= mash(seed);
		  if (me.s2 < 0) { me.s2 += 1; }
		  mash = null;
		}

		function copy(f, t) {
		  t.c = f.c;
		  t.s0 = f.s0;
		  t.s1 = f.s1;
		  t.s2 = f.s2;
		  return t;
		}

		function impl(seed, opts) {
		  var xg = new Alea(seed),
		      state = opts && opts.state,
		      prng = xg.next;
		  prng.int32 = function() { return (xg.next() * 0x100000000) | 0; };
		  prng.double = function() {
		    return prng() + (prng() * 0x200000 | 0) * 1.1102230246251565e-16; // 2^-53
		  };
		  prng.quick = prng;
		  if (state) {
		    if (typeof(state) == 'object') copy(state, xg);
		    prng.state = function() { return copy(xg, {}); };
		  }
		  return prng;
		}

		function Mash() {
		  var n = 0xefc8249d;

		  var mash = function(data) {
		    data = String(data);
		    for (var i = 0; i < data.length; i++) {
		      n += data.charCodeAt(i);
		      var h = 0.02519603282416938 * n;
		      n = h >>> 0;
		      h -= n;
		      h *= n;
		      n = h >>> 0;
		      h -= n;
		      n += h * 0x100000000; // 2^32
		    }
		    return (n >>> 0) * 2.3283064365386963e-10; // 2^-32
		  };

		  return mash;
		}


		if (module && module.exports) {
		  module.exports = impl;
		} else if (define && define.amd) {
		  define(function() { return impl; });
		} else {
		  this.alea = impl;
		}

		})(
		  commonjsGlobal,
		  module,    // present in node.js
		  (typeof undefined) == 'function'    // present with an AMD loader
		); 
	} (alea$1));

	var aleaExports = alea$1.exports;

	var xor128$1 = {exports: {}};

	(function (module) {
		// A Javascript implementaion of the "xor128" prng algorithm by
		// George Marsaglia.  See http://www.jstatsoft.org/v08/i14/paper

		(function(global, module, define) {

		function XorGen(seed) {
		  var me = this, strseed = '';

		  me.x = 0;
		  me.y = 0;
		  me.z = 0;
		  me.w = 0;

		  // Set up generator function.
		  me.next = function() {
		    var t = me.x ^ (me.x << 11);
		    me.x = me.y;
		    me.y = me.z;
		    me.z = me.w;
		    return me.w ^= (me.w >>> 19) ^ t ^ (t >>> 8);
		  };

		  if (seed === (seed | 0)) {
		    // Integer seed.
		    me.x = seed;
		  } else {
		    // String seed.
		    strseed += seed;
		  }

		  // Mix in string seed, then discard an initial batch of 64 values.
		  for (var k = 0; k < strseed.length + 64; k++) {
		    me.x ^= strseed.charCodeAt(k) | 0;
		    me.next();
		  }
		}

		function copy(f, t) {
		  t.x = f.x;
		  t.y = f.y;
		  t.z = f.z;
		  t.w = f.w;
		  return t;
		}

		function impl(seed, opts) {
		  var xg = new XorGen(seed),
		      state = opts && opts.state,
		      prng = function() { return (xg.next() >>> 0) / 0x100000000; };
		  prng.double = function() {
		    do {
		      var top = xg.next() >>> 11,
		          bot = (xg.next() >>> 0) / 0x100000000,
		          result = (top + bot) / (1 << 21);
		    } while (result === 0);
		    return result;
		  };
		  prng.int32 = xg.next;
		  prng.quick = prng;
		  if (state) {
		    if (typeof(state) == 'object') copy(state, xg);
		    prng.state = function() { return copy(xg, {}); };
		  }
		  return prng;
		}

		if (module && module.exports) {
		  module.exports = impl;
		} else if (define && define.amd) {
		  define(function() { return impl; });
		} else {
		  this.xor128 = impl;
		}

		})(
		  commonjsGlobal,
		  module,    // present in node.js
		  (typeof undefined) == 'function'    // present with an AMD loader
		); 
	} (xor128$1));

	var xor128Exports = xor128$1.exports;

	var xorwow$1 = {exports: {}};

	(function (module) {
		// A Javascript implementaion of the "xorwow" prng algorithm by
		// George Marsaglia.  See http://www.jstatsoft.org/v08/i14/paper

		(function(global, module, define) {

		function XorGen(seed) {
		  var me = this, strseed = '';

		  // Set up generator function.
		  me.next = function() {
		    var t = (me.x ^ (me.x >>> 2));
		    me.x = me.y; me.y = me.z; me.z = me.w; me.w = me.v;
		    return (me.d = (me.d + 362437 | 0)) +
		       (me.v = (me.v ^ (me.v << 4)) ^ (t ^ (t << 1))) | 0;
		  };

		  me.x = 0;
		  me.y = 0;
		  me.z = 0;
		  me.w = 0;
		  me.v = 0;

		  if (seed === (seed | 0)) {
		    // Integer seed.
		    me.x = seed;
		  } else {
		    // String seed.
		    strseed += seed;
		  }

		  // Mix in string seed, then discard an initial batch of 64 values.
		  for (var k = 0; k < strseed.length + 64; k++) {
		    me.x ^= strseed.charCodeAt(k) | 0;
		    if (k == strseed.length) {
		      me.d = me.x << 10 ^ me.x >>> 4;
		    }
		    me.next();
		  }
		}

		function copy(f, t) {
		  t.x = f.x;
		  t.y = f.y;
		  t.z = f.z;
		  t.w = f.w;
		  t.v = f.v;
		  t.d = f.d;
		  return t;
		}

		function impl(seed, opts) {
		  var xg = new XorGen(seed),
		      state = opts && opts.state,
		      prng = function() { return (xg.next() >>> 0) / 0x100000000; };
		  prng.double = function() {
		    do {
		      var top = xg.next() >>> 11,
		          bot = (xg.next() >>> 0) / 0x100000000,
		          result = (top + bot) / (1 << 21);
		    } while (result === 0);
		    return result;
		  };
		  prng.int32 = xg.next;
		  prng.quick = prng;
		  if (state) {
		    if (typeof(state) == 'object') copy(state, xg);
		    prng.state = function() { return copy(xg, {}); };
		  }
		  return prng;
		}

		if (module && module.exports) {
		  module.exports = impl;
		} else if (define && define.amd) {
		  define(function() { return impl; });
		} else {
		  this.xorwow = impl;
		}

		})(
		  commonjsGlobal,
		  module,    // present in node.js
		  (typeof undefined) == 'function'    // present with an AMD loader
		); 
	} (xorwow$1));

	var xorwowExports = xorwow$1.exports;

	var xorshift7$1 = {exports: {}};

	(function (module) {
		// A Javascript implementaion of the "xorshift7" algorithm by
		// François Panneton and Pierre L'ecuyer:
		// "On the Xorgshift Random Number Generators"
		// http://saluc.engr.uconn.edu/refs/crypto/rng/panneton05onthexorshift.pdf

		(function(global, module, define) {

		function XorGen(seed) {
		  var me = this;

		  // Set up generator function.
		  me.next = function() {
		    // Update xor generator.
		    var X = me.x, i = me.i, t, v;
		    t = X[i]; t ^= (t >>> 7); v = t ^ (t << 24);
		    t = X[(i + 1) & 7]; v ^= t ^ (t >>> 10);
		    t = X[(i + 3) & 7]; v ^= t ^ (t >>> 3);
		    t = X[(i + 4) & 7]; v ^= t ^ (t << 7);
		    t = X[(i + 7) & 7]; t = t ^ (t << 13); v ^= t ^ (t << 9);
		    X[i] = v;
		    me.i = (i + 1) & 7;
		    return v;
		  };

		  function init(me, seed) {
		    var j, X = [];

		    if (seed === (seed | 0)) {
		      // Seed state array using a 32-bit integer.
		      X[0] = seed;
		    } else {
		      // Seed state using a string.
		      seed = '' + seed;
		      for (j = 0; j < seed.length; ++j) {
		        X[j & 7] = (X[j & 7] << 15) ^
		            (seed.charCodeAt(j) + X[(j + 1) & 7] << 13);
		      }
		    }
		    // Enforce an array length of 8, not all zeroes.
		    while (X.length < 8) X.push(0);
		    for (j = 0; j < 8 && X[j] === 0; ++j);
		    if (j == 8) X[7] = -1; else X[j];

		    me.x = X;
		    me.i = 0;

		    // Discard an initial 256 values.
		    for (j = 256; j > 0; --j) {
		      me.next();
		    }
		  }

		  init(me, seed);
		}

		function copy(f, t) {
		  t.x = f.x.slice();
		  t.i = f.i;
		  return t;
		}

		function impl(seed, opts) {
		  if (seed == null) seed = +(new Date);
		  var xg = new XorGen(seed),
		      state = opts && opts.state,
		      prng = function() { return (xg.next() >>> 0) / 0x100000000; };
		  prng.double = function() {
		    do {
		      var top = xg.next() >>> 11,
		          bot = (xg.next() >>> 0) / 0x100000000,
		          result = (top + bot) / (1 << 21);
		    } while (result === 0);
		    return result;
		  };
		  prng.int32 = xg.next;
		  prng.quick = prng;
		  if (state) {
		    if (state.x) copy(state, xg);
		    prng.state = function() { return copy(xg, {}); };
		  }
		  return prng;
		}

		if (module && module.exports) {
		  module.exports = impl;
		} else if (define && define.amd) {
		  define(function() { return impl; });
		} else {
		  this.xorshift7 = impl;
		}

		})(
		  commonjsGlobal,
		  module,    // present in node.js
		  (typeof undefined) == 'function'    // present with an AMD loader
		); 
	} (xorshift7$1));

	var xorshift7Exports = xorshift7$1.exports;

	var xor4096$1 = {exports: {}};

	(function (module) {
		// A Javascript implementaion of Richard Brent's Xorgens xor4096 algorithm.
		//
		// This fast non-cryptographic random number generator is designed for
		// use in Monte-Carlo algorithms. It combines a long-period xorshift
		// generator with a Weyl generator, and it passes all common batteries
		// of stasticial tests for randomness while consuming only a few nanoseconds
		// for each prng generated.  For background on the generator, see Brent's
		// paper: "Some long-period random number generators using shifts and xors."
		// http://arxiv.org/pdf/1004.3115v1.pdf
		//
		// Usage:
		//
		// var xor4096 = require('xor4096');
		// random = xor4096(1);                        // Seed with int32 or string.
		// assert.equal(random(), 0.1520436450538547); // (0, 1) range, 53 bits.
		// assert.equal(random.int32(), 1806534897);   // signed int32, 32 bits.
		//
		// For nonzero numeric keys, this impelementation provides a sequence
		// identical to that by Brent's xorgens 3 implementaion in C.  This
		// implementation also provides for initalizing the generator with
		// string seeds, or for saving and restoring the state of the generator.
		//
		// On Chrome, this prng benchmarks about 2.1 times slower than
		// Javascript's built-in Math.random().

		(function(global, module, define) {

		function XorGen(seed) {
		  var me = this;

		  // Set up generator function.
		  me.next = function() {
		    var w = me.w,
		        X = me.X, i = me.i, t, v;
		    // Update Weyl generator.
		    me.w = w = (w + 0x61c88647) | 0;
		    // Update xor generator.
		    v = X[(i + 34) & 127];
		    t = X[i = ((i + 1) & 127)];
		    v ^= v << 13;
		    t ^= t << 17;
		    v ^= v >>> 15;
		    t ^= t >>> 12;
		    // Update Xor generator array state.
		    v = X[i] = v ^ t;
		    me.i = i;
		    // Result is the combination.
		    return (v + (w ^ (w >>> 16))) | 0;
		  };

		  function init(me, seed) {
		    var t, v, i, j, w, X = [], limit = 128;
		    if (seed === (seed | 0)) {
		      // Numeric seeds initialize v, which is used to generates X.
		      v = seed;
		      seed = null;
		    } else {
		      // String seeds are mixed into v and X one character at a time.
		      seed = seed + '\0';
		      v = 0;
		      limit = Math.max(limit, seed.length);
		    }
		    // Initialize circular array and weyl value.
		    for (i = 0, j = -32; j < limit; ++j) {
		      // Put the unicode characters into the array, and shuffle them.
		      if (seed) v ^= seed.charCodeAt((j + 32) % seed.length);
		      // After 32 shuffles, take v as the starting w value.
		      if (j === 0) w = v;
		      v ^= v << 10;
		      v ^= v >>> 15;
		      v ^= v << 4;
		      v ^= v >>> 13;
		      if (j >= 0) {
		        w = (w + 0x61c88647) | 0;     // Weyl.
		        t = (X[j & 127] ^= (v + w));  // Combine xor and weyl to init array.
		        i = (0 == t) ? i + 1 : 0;     // Count zeroes.
		      }
		    }
		    // We have detected all zeroes; make the key nonzero.
		    if (i >= 128) {
		      X[(seed && seed.length || 0) & 127] = -1;
		    }
		    // Run the generator 512 times to further mix the state before using it.
		    // Factoring this as a function slows the main generator, so it is just
		    // unrolled here.  The weyl generator is not advanced while warming up.
		    i = 127;
		    for (j = 4 * 128; j > 0; --j) {
		      v = X[(i + 34) & 127];
		      t = X[i = ((i + 1) & 127)];
		      v ^= v << 13;
		      t ^= t << 17;
		      v ^= v >>> 15;
		      t ^= t >>> 12;
		      X[i] = v ^ t;
		    }
		    // Storing state as object members is faster than using closure variables.
		    me.w = w;
		    me.X = X;
		    me.i = i;
		  }

		  init(me, seed);
		}

		function copy(f, t) {
		  t.i = f.i;
		  t.w = f.w;
		  t.X = f.X.slice();
		  return t;
		}
		function impl(seed, opts) {
		  if (seed == null) seed = +(new Date);
		  var xg = new XorGen(seed),
		      state = opts && opts.state,
		      prng = function() { return (xg.next() >>> 0) / 0x100000000; };
		  prng.double = function() {
		    do {
		      var top = xg.next() >>> 11,
		          bot = (xg.next() >>> 0) / 0x100000000,
		          result = (top + bot) / (1 << 21);
		    } while (result === 0);
		    return result;
		  };
		  prng.int32 = xg.next;
		  prng.quick = prng;
		  if (state) {
		    if (state.X) copy(state, xg);
		    prng.state = function() { return copy(xg, {}); };
		  }
		  return prng;
		}

		if (module && module.exports) {
		  module.exports = impl;
		} else if (define && define.amd) {
		  define(function() { return impl; });
		} else {
		  this.xor4096 = impl;
		}

		})(
		  commonjsGlobal,                                     // window object or global
		  module,    // present in node.js
		  (typeof undefined) == 'function'    // present with an AMD loader
		); 
	} (xor4096$1));

	var xor4096Exports = xor4096$1.exports;

	var tychei$1 = {exports: {}};

	(function (module) {
		// A Javascript implementaion of the "Tyche-i" prng algorithm by
		// Samuel Neves and Filipe Araujo.
		// See https://eden.dei.uc.pt/~sneves/pubs/2011-snfa2.pdf

		(function(global, module, define) {

		function XorGen(seed) {
		  var me = this, strseed = '';

		  // Set up generator function.
		  me.next = function() {
		    var b = me.b, c = me.c, d = me.d, a = me.a;
		    b = (b << 25) ^ (b >>> 7) ^ c;
		    c = (c - d) | 0;
		    d = (d << 24) ^ (d >>> 8) ^ a;
		    a = (a - b) | 0;
		    me.b = b = (b << 20) ^ (b >>> 12) ^ c;
		    me.c = c = (c - d) | 0;
		    me.d = (d << 16) ^ (c >>> 16) ^ a;
		    return me.a = (a - b) | 0;
		  };

		  /* The following is non-inverted tyche, which has better internal
		   * bit diffusion, but which is about 25% slower than tyche-i in JS.
		  me.next = function() {
		    var a = me.a, b = me.b, c = me.c, d = me.d;
		    a = (me.a + me.b | 0) >>> 0;
		    d = me.d ^ a; d = d << 16 ^ d >>> 16;
		    c = me.c + d | 0;
		    b = me.b ^ c; b = b << 12 ^ d >>> 20;
		    me.a = a = a + b | 0;
		    d = d ^ a; me.d = d = d << 8 ^ d >>> 24;
		    me.c = c = c + d | 0;
		    b = b ^ c;
		    return me.b = (b << 7 ^ b >>> 25);
		  }
		  */

		  me.a = 0;
		  me.b = 0;
		  me.c = 2654435769 | 0;
		  me.d = 1367130551;

		  if (seed === Math.floor(seed)) {
		    // Integer seed.
		    me.a = (seed / 0x100000000) | 0;
		    me.b = seed | 0;
		  } else {
		    // String seed.
		    strseed += seed;
		  }

		  // Mix in string seed, then discard an initial batch of 64 values.
		  for (var k = 0; k < strseed.length + 20; k++) {
		    me.b ^= strseed.charCodeAt(k) | 0;
		    me.next();
		  }
		}

		function copy(f, t) {
		  t.a = f.a;
		  t.b = f.b;
		  t.c = f.c;
		  t.d = f.d;
		  return t;
		}
		function impl(seed, opts) {
		  var xg = new XorGen(seed),
		      state = opts && opts.state,
		      prng = function() { return (xg.next() >>> 0) / 0x100000000; };
		  prng.double = function() {
		    do {
		      var top = xg.next() >>> 11,
		          bot = (xg.next() >>> 0) / 0x100000000,
		          result = (top + bot) / (1 << 21);
		    } while (result === 0);
		    return result;
		  };
		  prng.int32 = xg.next;
		  prng.quick = prng;
		  if (state) {
		    if (typeof(state) == 'object') copy(state, xg);
		    prng.state = function() { return copy(xg, {}); };
		  }
		  return prng;
		}

		if (module && module.exports) {
		  module.exports = impl;
		} else if (define && define.amd) {
		  define(function() { return impl; });
		} else {
		  this.tychei = impl;
		}

		})(
		  commonjsGlobal,
		  module,    // present in node.js
		  (typeof undefined) == 'function'    // present with an AMD loader
		); 
	} (tychei$1));

	var tycheiExports = tychei$1.exports;

	var seedrandom$1 = {exports: {}};

	var _nodeResolve_empty = {};

	var _nodeResolve_empty$1 = {
		__proto__: null,
		default: _nodeResolve_empty
	};

	var require$$0 = /*@__PURE__*/getAugmentedNamespace(_nodeResolve_empty$1);

	/*
	Copyright 2019 David Bau.

	Permission is hereby granted, free of charge, to any person obtaining
	a copy of this software and associated documentation files (the
	"Software"), to deal in the Software without restriction, including
	without limitation the rights to use, copy, modify, merge, publish,
	distribute, sublicense, and/or sell copies of the Software, and to
	permit persons to whom the Software is furnished to do so, subject to
	the following conditions:

	The above copyright notice and this permission notice shall be
	included in all copies or substantial portions of the Software.

	THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
	EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
	MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
	IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
	CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
	TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
	SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

	*/

	(function (module) {
		(function (global, pool, math) {
		//
		// The following constants are related to IEEE 754 limits.
		//

		var width = 256,        // each RC4 output is 0 <= x < 256
		    chunks = 6,         // at least six RC4 outputs for each double
		    digits = 52,        // there are 52 significant digits in a double
		    rngname = 'random', // rngname: name for Math.random and Math.seedrandom
		    startdenom = math.pow(width, chunks),
		    significance = math.pow(2, digits),
		    overflow = significance * 2,
		    mask = width - 1,
		    nodecrypto;         // node.js crypto module, initialized at the bottom.

		//
		// seedrandom()
		// This is the seedrandom function described above.
		//
		function seedrandom(seed, options, callback) {
		  var key = [];
		  options = (options == true) ? { entropy: true } : (options || {});

		  // Flatten the seed string or build one from local entropy if needed.
		  var shortseed = mixkey(flatten(
		    options.entropy ? [seed, tostring(pool)] :
		    (seed == null) ? autoseed() : seed, 3), key);

		  // Use the seed to initialize an ARC4 generator.
		  var arc4 = new ARC4(key);

		  // This function returns a random double in [0, 1) that contains
		  // randomness in every bit of the mantissa of the IEEE 754 value.
		  var prng = function() {
		    var n = arc4.g(chunks),             // Start with a numerator n < 2 ^ 48
		        d = startdenom,                 //   and denominator d = 2 ^ 48.
		        x = 0;                          //   and no 'extra last byte'.
		    while (n < significance) {          // Fill up all significant digits by
		      n = (n + x) * width;              //   shifting numerator and
		      d *= width;                       //   denominator and generating a
		      x = arc4.g(1);                    //   new least-significant-byte.
		    }
		    while (n >= overflow) {             // To avoid rounding up, before adding
		      n /= 2;                           //   last byte, shift everything
		      d /= 2;                           //   right using integer math until
		      x >>>= 1;                         //   we have exactly the desired bits.
		    }
		    return (n + x) / d;                 // Form the number within [0, 1).
		  };

		  prng.int32 = function() { return arc4.g(4) | 0; };
		  prng.quick = function() { return arc4.g(4) / 0x100000000; };
		  prng.double = prng;

		  // Mix the randomness into accumulated entropy.
		  mixkey(tostring(arc4.S), pool);

		  // Calling convention: what to return as a function of prng, seed, is_math.
		  return (options.pass || callback ||
		      function(prng, seed, is_math_call, state) {
		        if (state) {
		          // Load the arc4 state from the given state if it has an S array.
		          if (state.S) { copy(state, arc4); }
		          // Only provide the .state method if requested via options.state.
		          prng.state = function() { return copy(arc4, {}); };
		        }

		        // If called as a method of Math (Math.seedrandom()), mutate
		        // Math.random because that is how seedrandom.js has worked since v1.0.
		        if (is_math_call) { math[rngname] = prng; return seed; }

		        // Otherwise, it is a newer calling convention, so return the
		        // prng directly.
		        else return prng;
		      })(
		  prng,
		  shortseed,
		  'global' in options ? options.global : (this == math),
		  options.state);
		}

		//
		// ARC4
		//
		// An ARC4 implementation.  The constructor takes a key in the form of
		// an array of at most (width) integers that should be 0 <= x < (width).
		//
		// The g(count) method returns a pseudorandom integer that concatenates
		// the next (count) outputs from ARC4.  Its return value is a number x
		// that is in the range 0 <= x < (width ^ count).
		//
		function ARC4(key) {
		  var t, keylen = key.length,
		      me = this, i = 0, j = me.i = me.j = 0, s = me.S = [];

		  // The empty key [] is treated as [0].
		  if (!keylen) { key = [keylen++]; }

		  // Set up S using the standard key scheduling algorithm.
		  while (i < width) {
		    s[i] = i++;
		  }
		  for (i = 0; i < width; i++) {
		    s[i] = s[j = mask & (j + key[i % keylen] + (t = s[i]))];
		    s[j] = t;
		  }

		  // The "g" method returns the next (count) outputs as one number.
		  (me.g = function(count) {
		    // Using instance members instead of closure state nearly doubles speed.
		    var t, r = 0,
		        i = me.i, j = me.j, s = me.S;
		    while (count--) {
		      t = s[i = mask & (i + 1)];
		      r = r * width + s[mask & ((s[i] = s[j = mask & (j + t)]) + (s[j] = t))];
		    }
		    me.i = i; me.j = j;
		    return r;
		    // For robust unpredictability, the function call below automatically
		    // discards an initial batch of values.  This is called RC4-drop[256].
		    // See http://google.com/search?q=rsa+fluhrer+response&btnI
		  })(width);
		}

		//
		// copy()
		// Copies internal state of ARC4 to or from a plain object.
		//
		function copy(f, t) {
		  t.i = f.i;
		  t.j = f.j;
		  t.S = f.S.slice();
		  return t;
		}
		//
		// flatten()
		// Converts an object tree to nested arrays of strings.
		//
		function flatten(obj, depth) {
		  var result = [], typ = (typeof obj), prop;
		  if (depth && typ == 'object') {
		    for (prop in obj) {
		      try { result.push(flatten(obj[prop], depth - 1)); } catch (e) {}
		    }
		  }
		  return (result.length ? result : typ == 'string' ? obj : obj + '\0');
		}

		//
		// mixkey()
		// Mixes a string seed into a key that is an array of integers, and
		// returns a shortened string seed that is equivalent to the result key.
		//
		function mixkey(seed, key) {
		  var stringseed = seed + '', smear, j = 0;
		  while (j < stringseed.length) {
		    key[mask & j] =
		      mask & ((smear ^= key[mask & j] * 19) + stringseed.charCodeAt(j++));
		  }
		  return tostring(key);
		}

		//
		// autoseed()
		// Returns an object for autoseeding, using window.crypto and Node crypto
		// module if available.
		//
		function autoseed() {
		  try {
		    var out;
		    if (nodecrypto && (out = nodecrypto.randomBytes)) {
		      // The use of 'out' to remember randomBytes makes tight minified code.
		      out = out(width);
		    } else {
		      out = new Uint8Array(width);
		      (global.crypto || global.msCrypto).getRandomValues(out);
		    }
		    return tostring(out);
		  } catch (e) {
		    var browser = global.navigator,
		        plugins = browser && browser.plugins;
		    return [+new Date, global, plugins, global.screen, tostring(pool)];
		  }
		}

		//
		// tostring()
		// Converts an array of charcodes to a string
		//
		function tostring(a) {
		  return String.fromCharCode.apply(0, a);
		}

		//
		// When seedrandom.js is loaded, we immediately mix a few bits
		// from the built-in RNG into the entropy pool.  Because we do
		// not want to interfere with deterministic PRNG state later,
		// seedrandom will not call math.random on its own again after
		// initialization.
		//
		mixkey(math.random(), pool);

		//
		// Nodejs and AMD support: export the implementation as a module using
		// either convention.
		//
		if (module.exports) {
		  module.exports = seedrandom;
		  // When in node.js, try using crypto package for autoseeding.
		  try {
		    nodecrypto = require$$0;
		  } catch (ex) {}
		} else {
		  // When included as a plain script, set up Math.seedrandom global.
		  math['seed' + rngname] = seedrandom;
		}


		// End anonymous scope, and pass initial values.
		})(
		  // global: `self` in browsers (including strict mode and web workers),
		  // otherwise `this` in Node and other environments
		  (typeof self !== 'undefined') ? self : commonjsGlobal,
		  [],     // pool: entropy pool starts empty
		  Math    // math: package containing random, pow, and seedrandom
		); 
	} (seedrandom$1));

	var seedrandomExports = seedrandom$1.exports;

	// A library of seedable RNGs implemented in Javascript.
	//
	// Usage:
	//
	// var seedrandom = require('seedrandom');
	// var random = seedrandom(1); // or any seed.
	// var x = random();       // 0 <= x < 1.  Every bit is random.
	// var x = random.quick(); // 0 <= x < 1.  32 bits of randomness.

	// alea, a 53-bit multiply-with-carry generator by Johannes Baagøe.
	// Period: ~2^116
	// Reported to pass all BigCrush tests.
	var alea = aleaExports;

	// xor128, a pure xor-shift generator by George Marsaglia.
	// Period: 2^128-1.
	// Reported to fail: MatrixRank and LinearComp.
	var xor128 = xor128Exports;

	// xorwow, George Marsaglia's 160-bit xor-shift combined plus weyl.
	// Period: 2^192-2^32
	// Reported to fail: CollisionOver, SimpPoker, and LinearComp.
	var xorwow = xorwowExports;

	// xorshift7, by François Panneton and Pierre L'ecuyer, takes
	// a different approach: it adds robustness by allowing more shifts
	// than Marsaglia's original three.  It is a 7-shift generator
	// with 256 bits, that passes BigCrush with no systmatic failures.
	// Period 2^256-1.
	// No systematic BigCrush failures reported.
	var xorshift7 = xorshift7Exports;

	// xor4096, by Richard Brent, is a 4096-bit xor-shift with a
	// very long period that also adds a Weyl generator. It also passes
	// BigCrush with no systematic failures.  Its long period may
	// be useful if you have many generators and need to avoid
	// collisions.
	// Period: 2^4128-2^32.
	// No systematic BigCrush failures reported.
	var xor4096 = xor4096Exports;

	// Tyche-i, by Samuel Neves and Filipe Araujo, is a bit-shifting random
	// number generator derived from ChaCha, a modern stream cipher.
	// https://eden.dei.uc.pt/~sneves/pubs/2011-snfa2.pdf
	// Period: ~2^127
	// No systematic BigCrush failures reported.
	var tychei = tycheiExports;

	// The original ARC4-based prng included in this library.
	// Period: ~2^1600
	var sr = seedrandomExports;

	sr.alea = alea;
	sr.xor128 = xor128;
	sr.xorwow = xorwow;
	sr.xorshift7 = xorshift7;
	sr.xor4096 = xor4096;
	sr.tychei = tychei;

	var seedrandom = sr;

	/**
	 * @license
	 * Copyright 2018 Google LLC. All Rights Reserved.
	 * Licensed under the Apache License, Version 2.0 (the "License");
	 * you may not use this file except in compliance with the License.
	 * You may obtain a copy of the License at
	 *
	 * http://www.apache.org/licenses/LICENSE-2.0
	 *
	 * Unless required by applicable law or agreed to in writing, software
	 * distributed under the License is distributed on an "AS IS" BASIS,
	 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
	 * See the License for the specific language governing permissions and
	 * limitations under the License.
	 *
	 * =============================================================================
	 */
	/**
	 * Apply a mapping function to a nested structure in a recursive manner.
	 *
	 * The result of the mapping is an object with the same nested structure (i.e.,
	 * of arrays and dicts) as the input, except that some subtrees are replaced,
	 * according to the results of the mapping function.
	 *
	 * Mappings are memoized.  Thus, if the nested structure contains the same
	 * object in multiple positions, the output will contain the same mapped object
	 * in those positions.  Cycles are not supported, however.
	 *
	 * @param input: The object to which to apply the mapping function.
	 * @param mapFn: A function that expects a single node of the object tree, and
	 *   returns a `DeepMapResult`.  The `DeepMapResult` either provides a
	 *   replacement value for that node (i.e., replacing the subtree), or indicates
	 *   that the node should be processed recursively.
	 */
	function deepMap(input, mapFn) {
	    return deepMapInternal(input, mapFn);
	}
	/**
	 * @param seen: A Map of known object mappings (i.e., memoized results of
	 *   `mapFn()`)
	 * @param containedIn: An set containing objects on the reference path currently
	 *   being processed (used to detect cycles).
	 */
	function deepMapInternal(input, mapFn, seen = new Map(), containedIn = new Set()) {
	    if (input == null) {
	        return null;
	    }
	    if (typeof Blob === 'function' && input instanceof Blob) {
	        return input.slice();
	    }
	    if (containedIn.has(input)) {
	        throw new Error('Circular references are not supported.');
	    }
	    if (seen.has(input)) {
	        return seen.get(input);
	    }
	    const result = mapFn(input);
	    if (result.recurse && result.value !== null) {
	        throw new Error('A deep map function may not return both a value and recurse=true.');
	    }
	    if (!result.recurse) {
	        seen.set(input, result.value);
	        return result.value;
	    }
	    else if (isIterable(input)) {
	        // tslint:disable-next-line:no-any
	        const mappedIterable = Array.isArray(input) ? [] : {};
	        containedIn.add(input);
	        for (const k in input) {
	            const child = input[k];
	            const childResult = deepMapInternal(child, mapFn, seen, containedIn);
	            mappedIterable[k] = childResult;
	        }
	        containedIn.delete(input);
	        if (input.__proto__) {
	            mappedIterable.__proto__ = input.__proto__;
	        }
	        return mappedIterable;
	    }
	    else {
	        throw new Error(`Can't recurse into non-iterable type: ${input}`);
	    }
	}
	// TODO(soergel, kangyizhang) Reconsider naming of deepZip() to avoid confusion
	// with zip()
	/**
	 * Zip nested structures together in a recursive manner.
	 *
	 * This has the effect of transposing or pivoting data, e.g. converting it from
	 * a row-major representation to a column-major representation.
	 *
	 * For example, `deepZip([{a: 1, b: 2}, {a: 3, b: 4}])` returns
	 * `{a: [1, 3], b: [2, 4]}`.
	 *
	 * The inputs should all have the same nested structure (i.e., of arrays and
	 * dicts).  The result is a single object with the same nested structure, where
	 * the leaves are arrays collecting the values of the inputs at that location
	 * (or, optionally, the result of a custom function applied to those arrays).
	 *
	 * @param inputs: An array of the objects to zip together.
	 * @param zipFn: (optional) A function that expects an array of elements at a
	 *   single node of the object tree, and returns a `DeepMapResult`.  The
	 *   `DeepMapResult` either provides a result value for that node (i.e.,
	 *   representing the subtree), or indicates that the node should be processed
	 *   recursively.  The default zipFn recurses as far as possible and places
	 *   arrays at the leaves.
	 */
	function deepZip(inputs, zipFn = zipToList) {
	    return deepZipInternal(inputs, zipFn);
	}
	/**
	 * @param containedIn: An set containing objects on the reference path currently
	 *   being processed (used to detect cycles).
	 */
	function deepZipInternal(inputs, zipFn, containedIn = new Set()) {
	    // The recursion follows the structure of input 0; it's assumed that all the
	    // other inputs have the same structure.
	    const input = inputs[0];
	    if (containedIn.has(input)) {
	        throw new Error('Circular references are not supported.');
	    }
	    const result = zipFn(inputs);
	    if (result.recurse && result.value !== null) {
	        throw new Error('A deep zip function may not return both a value and recurse=true.');
	    }
	    if (!result.recurse) {
	        return result.value;
	    }
	    else if (isIterable(input)) {
	        // tslint:disable-next-line:no-any
	        const mappedIterable = Array.isArray(input) ? [] : {};
	        containedIn.add(input);
	        for (const k in input) {
	            const children = inputs.map(x => x[k]);
	            const childResult = deepZipInternal(children, zipFn, containedIn);
	            mappedIterable[k] = childResult;
	        }
	        containedIn.delete(input);
	        return mappedIterable;
	    }
	    else {
	        throw new Error(`Can't recurse into non-iterable type: ${input}`);
	    }
	}
	// tslint:disable-next-line:no-any
	function zipToList(x) {
	    if (x === null) {
	        return null;
	    }
	    // TODO(soergel): validate array type?
	    if (isIterable(x[0])) {
	        return { value: null, recurse: true };
	    }
	    else {
	        return { value: x, recurse: false };
	    }
	}
	/**
	 * Apply an async mapping function to a nested structure in a recursive manner.
	 *
	 * This first creates a nested structure of Promises, and then awaits all of
	 * those, resulting in a single Promise for a resolved nested structure.
	 *
	 * The result of the mapping is an object with the same nested structure (i.e.,
	 * of arrays and dicts) as the input, except that some subtrees are replaced,
	 * according to the results of the mapping function.
	 *
	 * Mappings are memoized.  Thus, if the nested structure contains the same
	 * object in multiple positions, the output will contain the same mapped object
	 * in those positions.  Cycles are not supported, however.
	 *
	 * @param input: The object to which to apply the mapping function.
	 * @param mapFn: A function that expects a single node of the object tree, and
	 *   returns a `DeepMapAsyncResult`.  The `DeepMapAsyncResult` either provides
	 *   a `Promise` for a replacement value for that node (i.e., replacing the
	 *   subtree), or indicates that the node should be processed recursively.  Note
	 *   that the decision whether or not to recurse must be made immediately; only
	 *   the mapped value may be promised.
	 */
	async function deepMapAndAwaitAll(input, mapFn) {
	    const seen = new Map();
	    // First do a normal deepMap, collecting Promises in 'seen' as a side effect.
	    deepMapInternal(input, mapFn, seen);
	    // Replace the Promises in 'seen' in place.
	    // Note TypeScript provides no async map iteration, and regular map iteration
	    // is broken too, so sadly we have to do Array.from() to make it work.
	    // (There's no advantage to Promise.all(), and that would be tricky anyway.)
	    for (const key of Array.from(seen.keys())) {
	        const value = seen.get(key);
	        if (tf__namespace.util.isPromise(value)) {
	            const mappedValue = await value;
	            seen.set(key, mappedValue);
	        }
	    }
	    // Normal deepMap again, this time filling in the resolved values.
	    // It's unfortunate that we have to do two passes.
	    // TODO(soergel): test performance and think harder about a fast solution.
	    const result = deepMapInternal(input, mapFn, seen);
	    return result;
	}
	/**
	 * Determine whether the argument is iterable.
	 *
	 * @returns true if the argument is an array or any non-Tensor object.
	 */
	// tslint:disable-next-line:no-any
	function isIterable(obj) {
	    let isTextDecoder = false;
	    if (tf__namespace.env().get('IS_BROWSER')) {
	        isTextDecoder = obj instanceof TextDecoder;
	    }
	    else {
	        // tslint:disable-next-line:no-require-imports
	        const { StringDecoder } = require('string_decoder');
	        isTextDecoder = obj instanceof StringDecoder;
	    }
	    return obj != null && (!ArrayBuffer.isView(obj)) &&
	        (Array.isArray(obj) ||
	            (typeof obj === 'object' && !(obj instanceof tf__namespace.Tensor) &&
	                !(obj instanceof Promise) && !isTextDecoder));
	}
	/**
	 * Determine whether the argument can be converted to Tensor.
	 *
	 * Tensors, primitives, arrays, and TypedArrays all qualify; anything else does
	 * not.
	 *
	 * @returns true if the argument can be converted to Tensor.
	 */
	// tslint:disable-next-line:no-any
	function canTensorify(obj) {
	    return obj == null || isPrimitive(obj) || Array.isArray(obj) ||
	        (typeof obj === 'object' && (obj instanceof tf__namespace.Tensor)) ||
	        tf__namespace.util.isTypedArray(obj);
	}
	/**
	 * Returns true if the given `value` is a primitive type. Otherwise returns
	 * false. This is equivalant to node util.isPrimitive
	 */
	function isPrimitive(value) {
	    return (value === null ||
	        (typeof value !== 'object' && typeof value !== 'function'));
	}

	/**
	 * @license
	 * Copyright 2018 Google LLC. All Rights Reserved.
	 * Licensed under the Apache License, Version 2.0 (the "License");
	 * you may not use this file except in compliance with the License.
	 * You may obtain a copy of the License at
	 *
	 * http://www.apache.org/licenses/LICENSE-2.0
	 *
	 * Unless required by applicable law or agreed to in writing, software
	 * distributed under the License is distributed on an "AS IS" BASIS,
	 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
	 * See the License for the specific language governing permissions and
	 * limitations under the License.
	 *
	 * =============================================================================
	 */
	function deepClone(container) {
	    return deepMap(container, cloneIfTensor);
	}
	// tslint:disable-next-line: no-any
	function cloneIfTensor(item) {
	    if (item instanceof tf__namespace.Tensor) {
	        return ({ value: item.clone(), recurse: false });
	    }
	    else if (isIterable(item)) {
	        return { value: null, recurse: true };
	    }
	    else {
	        return { value: item, recurse: false };
	    }
	}

	/**
	 * @license
	 * Copyright 2018 Google LLC. All Rights Reserved.
	 * Licensed under the Apache License, Version 2.0 (the "License");
	 * you may not use this file except in compliance with the License.
	 * You may obtain a copy of the License at
	 *
	 * http://www.apache.org/licenses/LICENSE-2.0
	 *
	 * Unless required by applicable law or agreed to in writing, software
	 * distributed under the License is distributed on an "AS IS" BASIS,
	 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
	 * See the License for the specific language governing permissions and
	 * limitations under the License.
	 *
	 * =============================================================================
	 */
	/**
	 * A ring buffer, providing O(1) FIFO, LIFO, and related operations.
	 */
	class RingBuffer {
	    /**
	     * Constructs a `RingBuffer`.
	     * @param capacity The number of items that the buffer can accomodate.
	     */
	    constructor(capacity) {
	        this.capacity = capacity;
	        // Note we store the indices in the range 0 <= index < 2*capacity.
	        // This allows us to distinguish the full from the empty case.
	        // See https://www.snellman.net/blog/archive/2016-12-13-ring-buffers/
	        this.begin = 0; // inclusive
	        this.end = 0; // exclusive
	        if (capacity == null) {
	            throw new RangeError('Can\'t create a ring buffer of unknown capacity.');
	        }
	        if (capacity < 1) {
	            throw new RangeError('Can\'t create ring buffer of capacity < 1.');
	        }
	        this.data = new Array(capacity);
	        this.doubledCapacity = 2 * capacity;
	    }
	    /**
	     * Map any index into the range 0 <= index < 2*capacity.
	     */
	    wrap(index) {
	        // don't trust % on negative numbers
	        while (index < 0) {
	            index += this.doubledCapacity;
	        }
	        return index % this.doubledCapacity;
	    }
	    get(index) {
	        if (index < 0) {
	            throw new RangeError('Can\'t get item at a negative index.');
	        }
	        return this.data[index % this.capacity];
	    }
	    set(index, value) {
	        if (index < 0) {
	            throw new RangeError('Can\'t set item at a negative index.');
	        }
	        this.data[index % this.capacity] = value;
	    }
	    /**
	     * Returns the current number of items in the buffer.
	     */
	    length() {
	        let length = this.end - this.begin;
	        if (length < 0) {
	            length = this.doubledCapacity + length;
	        }
	        return length;
	    }
	    /**
	     * Reports whether the buffer is full.
	     * @returns true if the number of items in the buffer equals its capacity, and
	     *   false otherwise.
	     */
	    isFull() {
	        return this.length() === this.capacity;
	    }
	    /**
	     * Reports whether the buffer is empty.
	     * @returns true if the number of items in the buffer equals zero, and
	     *   false otherwise.
	     */
	    isEmpty() {
	        return this.length() === 0;
	    }
	    /**
	     * Adds an item to the end of the buffer.
	     */
	    push(value) {
	        if (this.isFull()) {
	            throw new RangeError('Ring buffer is full.');
	        }
	        this.set(this.end, value);
	        this.end = this.wrap(this.end + 1);
	    }
	    /**
	     * Adds many items to the end of the buffer, in order.
	     */
	    pushAll(values) {
	        for (const value of values) {
	            this.push(value);
	        }
	    }
	    /**
	     * Removes and returns the last item in the buffer.
	     */
	    pop() {
	        if (this.isEmpty()) {
	            throw new RangeError('Ring buffer is empty.');
	        }
	        this.end = this.wrap(this.end - 1);
	        const result = this.get(this.end);
	        this.set(this.end, undefined);
	        return result;
	    }
	    /**
	     * Adds an item to the beginning of the buffer.
	     */
	    unshift(value) {
	        if (this.isFull()) {
	            throw new RangeError('Ring buffer is full.');
	        }
	        this.begin = this.wrap(this.begin - 1);
	        this.set(this.begin, value);
	    }
	    /**
	     * Removes and returns the first item in the buffer.
	     */
	    shift() {
	        if (this.isEmpty()) {
	            throw new RangeError('Ring buffer is empty.');
	        }
	        const result = this.get(this.begin);
	        this.set(this.begin, undefined);
	        this.begin = this.wrap(this.begin + 1);
	        return result;
	    }
	    /**
	     * Removes and returns a specific item in the buffer, and moves the last item
	     * to the vacated slot.  This is useful for implementing a shuffling stream.
	     * Note that this operation necessarily scrambles the original order.
	     *
	     * @param relativeIndex: the index of the item to remove, relative to the
	     *   first item in the buffer (e.g., hiding the ring nature of the underlying
	     *   storage).
	     */
	    shuffleExcise(relativeIndex) {
	        if (this.isEmpty()) {
	            throw new RangeError('Ring buffer is empty.');
	        }
	        const index = this.wrap(this.begin + relativeIndex);
	        const result = this.get(index);
	        this.set(index, this.pop());
	        return result;
	    }
	}

	/**
	 * @license
	 * Copyright 2018 Google LLC. All Rights Reserved.
	 * Licensed under the Apache License, Version 2.0 (the "License");
	 * you may not use this file except in compliance with the License.
	 * You may obtain a copy of the License at
	 *
	 * http://www.apache.org/licenses/LICENSE-2.0
	 *
	 * Unless required by applicable law or agreed to in writing, software
	 * distributed under the License is distributed on an "AS IS" BASIS,
	 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
	 * See the License for the specific language governing permissions and
	 * limitations under the License.
	 *
	 * =============================================================================
	 */
	class GrowingRingBuffer extends RingBuffer {
	    /**
	     * Constructs a `GrowingRingBuffer`.
	     */
	    constructor() {
	        super(GrowingRingBuffer.INITIAL_CAPACITY);
	    }
	    isFull() {
	        return false;
	    }
	    push(value) {
	        if (super.isFull()) {
	            this.expand();
	        }
	        super.push(value);
	    }
	    unshift(value) {
	        if (super.isFull()) {
	            this.expand();
	        }
	        super.unshift(value);
	    }
	    /**
	     * Doubles the capacity of the buffer.
	     */
	    expand() {
	        const newCapacity = this.capacity * 2;
	        const newData = new Array(newCapacity);
	        const len = this.length();
	        // Rotate the buffer to start at index 0 again, since we can't just
	        // allocate more space at the end.
	        for (let i = 0; i < len; i++) {
	            newData[i] = this.get(this.wrap(this.begin + i));
	        }
	        this.data = newData;
	        this.capacity = newCapacity;
	        this.doubledCapacity = 2 * this.capacity;
	        this.begin = 0;
	        this.end = len;
	    }
	}
	GrowingRingBuffer.INITIAL_CAPACITY = 32;

	/**
	 * @license
	 * Copyright 2018 Google LLC. All Rights Reserved.
	 * Licensed under the Apache License, Version 2.0 (the "License");
	 * you may not use this file except in compliance with the License.
	 * You may obtain a copy of the License at
	 *
	 * http://www.apache.org/licenses/LICENSE-2.0
	 *
	 * Unless required by applicable law or agreed to in writing, software
	 * distributed under the License is distributed on an "AS IS" BASIS,
	 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
	 * See the License for the specific language governing permissions and
	 * limitations under the License.
	 *
	 * =============================================================================
	 */
	// Here we implement a simple asynchronous iterator.
	// This lets us avoid using either third-party stream libraries or
	// recent TypeScript language support requiring polyfills.
	/**
	 * Create a `LazyIterator` from an array of items.
	 */
	function iteratorFromItems(items) {
	    return new ArrayIterator(items);
	}
	/**
	 * Create a `LazyIterator` from a function.
	 *
	 * ```js
	 * let i = -1;
	 * const func = () =>
	 *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};
	 * const iter = tf.data.iteratorFromFunction(func);
	 * await iter.forEachAsync(e => console.log(e));
	 * ```
	 *
	 * @param func A function that produces data on each call.
	 */
	function iteratorFromFunction(func) {
	    return new FunctionCallIterator(func);
	}
	/**
	 * Create a `LazyIterator` by concatenating underlying streams, which are
	 * themselves provided as a stream.
	 *
	 * This can also be thought of as a "stream flatten" operation.
	 *
	 * @param baseIterators A stream of streams to be concatenated.
	 * @param baseErrorHandler An optional function that can intercept `Error`s
	 *   raised during a `next()` call on the base stream.  This function can decide
	 *   whether the error should be propagated, whether the error should be
	 *   ignored, or whether the base stream should be terminated.
	 */
	function iteratorFromConcatenated(baseIterators, baseErrorHandler) {
	    return new ChainedIterator(baseIterators, baseErrorHandler);
	}
	/**
	 * Create a `LazyIterator` by zipping together an array, dict, or nested
	 * structure of `LazyIterator`s (and perhaps additional constants).
	 *
	 * The underlying streams must provide elements in a consistent order such
	 * that they correspond.
	 *
	 * Typically, the underlying streams should have the same number of
	 * elements. If they do not, the behavior is determined by the
	 * `mismatchMode` argument.
	 *
	 * The nested structure of the `iterators` argument determines the
	 * structure of elements in the resulting iterator.
	 *
	 * @param iterators: An array or object containing LazyIterators at the
	 * leaves.
	 * @param mismatchMode: Determines what to do when one underlying iterator
	 * is exhausted before the others.  `ZipMismatchMode.FAIL` (the default)
	 * causes an error to be thrown in this case.  `ZipMismatchMode.SHORTEST`
	 * causes the zipped iterator to terminate with the furst underlying
	 * streams, so elements remaining on the longer streams are ignored.
	 * `ZipMismatchMode.LONGEST` causes the zipped stream to continue, filling
	 * in nulls for the exhausted streams, until all streams are exhausted.
	 */
	function iteratorFromZipped(iterators, mismatchMode = ZipMismatchMode.FAIL) {
	    return new ZipIterator(iterators, mismatchMode);
	}
	/**
	 * An asynchronous iterator, providing lazy access to a potentially
	 * unbounded stream of elements.
	 *
	 * Iterator can be obtained from a dataset:
	 * `const iter = await dataset.iterator();`
	 */
	class LazyIterator {
	    /**
	     * Collect all remaining elements of a bounded stream into an array.
	     * Obviously this will succeed only for small streams that fit in memory.
	     * Useful for testing.
	     *
	     * @returns A Promise for an array of stream elements, which will resolve
	     *   when the stream is exhausted.
	     */
	    async toArray() {
	        const result = [];
	        let x = await this.next();
	        while (!x.done) {
	            result.push(x.value);
	            x = await this.next();
	        }
	        return result;
	    }
	    /**
	     * Collect all elements of this dataset into an array with prefetching 100
	     * elements. This is useful for testing, because the prefetch changes the
	     * order in which the Promises are resolved along the processing pipeline.
	     * This may help expose bugs where results are dependent on the order of
	     * Promise resolution rather than on the logical order of the stream (i.e.,
	     * due to hidden mutable state).
	     *
	     * @returns A Promise for an array of stream elements, which will resolve
	     *   when the stream is exhausted.
	     */
	    async toArrayForTest() {
	        const stream = this.prefetch(100);
	        const result = [];
	        let x = await stream.next();
	        while (!x.done) {
	            result.push(x.value);
	            x = await stream.next();
	        }
	        return result;
	    }
	    /**
	     * Draw items from the stream until it is exhausted.
	     *
	     * This can be useful when the stream has side effects but no output.  In
	     * that case, calling this function guarantees that the stream will be
	     * fully processed.
	     */
	    async resolveFully() {
	        let x = await this.next();
	        while (!x.done) {
	            x = await this.next();
	        }
	    }
	    /**
	     * Draw items from the stream until it is exhausted, or a predicate fails.
	     *
	     * This can be useful when the stream has side effects but no output.  In
	     * that case, calling this function guarantees that the stream will be
	     * fully processed.
	     */
	    async resolveWhile(predicate) {
	        let x = await this.next();
	        let shouldContinue = predicate(x.value);
	        while ((!x.done) && shouldContinue) {
	            x = await this.next();
	            shouldContinue = predicate(x.value);
	        }
	    }
	    /**
	     * Handles errors thrown on this stream using a provided handler function.
	     *
	     * @param handler A function that handles any `Error` thrown during a `next()`
	     *   call and returns true if the stream should continue (dropping the failed
	     *   call) or false if the stream should quietly terminate.  If the handler
	     *   itself throws (or rethrows) an `Error`, that will be propagated.
	     *
	     * @returns A `LazyIterator` of elements passed through from upstream,
	     *   possibly filtering or terminating on upstream `next()` calls that
	     *   throw an `Error`.
	     */
	    handleErrors(handler) {
	        return new ErrorHandlingLazyIterator(this, handler);
	    }
	    // TODO(soergel): Implement reduce() etc.
	    /**
	     * Filters this stream according to `predicate`.
	     *
	     * @param predicate A function mapping a stream element to a boolean or a
	     * `Promise` for one.
	     *
	     * @returns A `LazyIterator` of elements for which the predicate was true.
	     */
	    filter(predicate) {
	        return new FilterIterator(this, predicate);
	    }
	    /**
	     * Maps this stream through a 1-to-1 transform.
	     *
	     * @param transform A function mapping a stream element to a transformed
	     *   element.
	     *
	     * @returns A `LazyIterator` of transformed elements.
	     */
	    map(transform) {
	        return new MapIterator(this, transform);
	    }
	    /**
	     * Maps this stream through an async 1-to-1 transform.
	     *
	     * @param transform A function mapping a stream element to a `Promise` for a
	     *   transformed stream element.
	     *
	     * @returns A `LazyIterator` of transformed elements.
	     */
	    mapAsync(transform) {
	        return new AsyncMapIterator(this, transform);
	    }
	    /**
	     * Maps this stream through a 1-to-1 transform, forcing serial execution.
	     *
	     * @param transform A function mapping a stream element to a transformed
	     *   element.
	     *
	     * @returns A `LazyIterator` of transformed elements.
	     */
	    serialMapAsync(transform) {
	        return new AsyncMapIterator(this, transform).serial();
	    }
	    /**
	     * Maps this stream through a 1-to-many transform.
	     *
	     * @param transform A function mapping a stream element to an array of
	     *   transformed elements.
	     *
	     * @returns A `DataStream` of transformed elements.
	     */
	    flatmap(transform) {
	        return new FlatmapIterator(this, transform);
	    }
	    /**
	     * Apply a function to every element of the stream.
	     *
	     * @param f A function to apply to each stream element.
	     */
	    async forEachAsync(f) {
	        return this.map(f).resolveFully();
	    }
	    /**
	     * Apply a function to every element of the stream, forcing serial execution.
	     *
	     * @param f A function to apply to each stream element.  Should return 'true'
	     *   to indicate that the stream should continue, or 'false' to cause it to
	     *   terminate.
	     */
	    async serialForEach(f) {
	        return this.serialMapAsync(f).resolveWhile(x => (x === true));
	    }
	    /**
	     * Groups elements into batches, represented as arrays of elements.
	     *
	     * We can think of the elements of this iterator as 'rows' (even if they are
	     * nested structures).  By the same token, consecutive values for a given
	     * key within the elements form a 'column'.  This matches the usual sense of
	     * 'row' and 'column' when processing tabular data (e.g., parsing a CSV).
	     *
	     * Thus, "Row-major" means that the resulting batch is simply a collection of
	     * rows: `[row1, row2, row3, ...]`.  This is contrast to the column-major
	     * form, which is needed for vectorized computation.
	     *
	     * @param batchSize The number of elements desired per batch.
	     * @param smallLastBatch Whether to emit the final batch when it has fewer
	     *   than batchSize elements. Default true.
	     * @returns A `LazyIterator` of batches of elements, represented as arrays
	     *   of the original element type.
	     */
	    rowMajorBatch(batchSize, smallLastBatch = true) {
	        return new RowMajorBatchIterator(this, batchSize, smallLastBatch);
	    }
	    /**
	     * Groups elements into batches, represented in column-major form.
	     *
	     * We can think of the elements of this iterator as 'rows' (even if they are
	     * nested structures).  By the same token, consecutive values for a given
	     * key within the elements form a 'column'.  This matches the usual sense of
	     * 'row' and 'column' when processing tabular data (e.g., parsing a CSV).
	     *
	     * Thus, "column-major" means that the resulting batch is a (potentially
	     * nested) structure representing the columns.  Each column entry, then,
	     * contains a collection of the values found in that column for a range of
	     * input elements.  This representation allows for vectorized computation, in
	     * contrast to the row-major form.
	     *
	     * The inputs should all have the same nested structure (i.e., of arrays and
	     * dicts).  The result is a single object with the same nested structure,
	     * where the leaves are arrays collecting the values of the inputs at that
	     * location (or, optionally, the result of a custom function applied to those
	     * arrays).
	     *
	     * @param batchSize The number of elements desired per batch.
	     * @param smallLastBatch Whether to emit the final batch when it has fewer
	     *   than batchSize elements. Default true.
	     * @param zipFn: (optional) A function that expects an array of elements at a
	     *   single node of the object tree, and returns a `DeepMapResult`.  The
	     *   `DeepMapResult` either provides a result value for that node (i.e.,
	     *   representing the subtree), or indicates that the node should be processed
	     *   recursively.  The default zipFn recurses as far as possible and places
	     *   arrays at the leaves.
	     * @returns A `LazyIterator` of batches of elements, represented as an object
	     *   with collections at the leaves.
	     */
	    columnMajorBatch(batchSize, smallLastBatch = true, 
	    // tslint:disable-next-line:no-any
	    zipFn = zipToList) {
	        // First collect the desired number of input elements as a row-major batch.
	        const rowBatches = this.rowMajorBatch(batchSize, smallLastBatch);
	        // Now 'rotate' or 'pivot' the data, collecting all values from each column
	        // in the batch (i.e., for each key within the elements) into an array.
	        return rowBatches.map(x => deepZip(x, zipFn));
	    }
	    /**
	     * Concatenate this `LazyIterator` with another.
	     *
	     * @param iterator A `LazyIterator` to be concatenated onto this one.
	     * @param baseErrorHandler An optional function that can intercept `Error`s
	     *   raised during a `next()` call on the base stream.  This function can
	     *   decide whether the error should be propagated, whether the error should
	     *   be ignored, or whether the base stream should be terminated.
	     * @returns A `LazyIterator`.
	     */
	    concatenate(iterator, baseErrorHandler) {
	        return new ChainedIterator(iteratorFromItems([this, iterator]), baseErrorHandler);
	    }
	    /**
	     * Limits this stream to return at most `count` items.
	     *
	     * @param count The maximum number of items to provide from the stream. If
	     * a negative or undefined value is given, the entire stream is returned
	     *   unaltered.
	     */
	    take(count) {
	        if (count < 0 || count == null) {
	            return this;
	        }
	        return new TakeIterator(this, count);
	    }
	    /**
	     * Skips the first `count` items in this stream.
	     *
	     * @param count The number of items to skip.  If a negative or undefined
	     * value is given, the entire stream is returned unaltered.
	     */
	    skip(count) {
	        if (count < 0 || count == null) {
	            return this;
	        }
	        return new SkipIterator(this, count);
	    }
	    /**
	     * Prefetch the first `bufferSize` items in this stream.
	     *
	     * Note this prefetches Promises, but makes no guarantees about when those
	     * Promises resolve.
	     *
	     * @param bufferSize: An integer specifying the number of elements to be
	     *   prefetched.
	     */
	    prefetch(bufferSize) {
	        return new PrefetchIterator(this, bufferSize);
	    }
	    // TODO(soergel): deep sharded shuffle, where supported
	    /**
	     * Randomly shuffles the elements of this stream.
	     *
	     * @param bufferSize: An integer specifying the number of elements from
	     * this stream from which the new stream will sample.
	     * @param seed: (Optional.) An integer specifying the random seed that
	     * will be used to create the distribution.
	     */
	    shuffle(windowSize, seed) {
	        return new ShuffleIterator(this, windowSize, seed);
	    }
	    /**
	     * Force an iterator to execute serially: each next() call will await the
	     * prior one, so that they cannot execute concurrently.
	     */
	    serial() {
	        return new SerialIterator(this);
	    }
	}
	// ============================================================================
	// The following private classes serve to implement the chainable methods
	// on LazyIterator.  Unfortunately they can't be placed in separate files,
	// due to resulting trouble with circular imports.
	// ============================================================================
	// Iterators that just extend LazyIterator directly
	// ============================================================================
	class ArrayIterator extends LazyIterator {
	    constructor(items) {
	        super();
	        this.items = items;
	        this.trav = 0;
	    }
	    summary() {
	        return `Array of ${this.items.length} items`;
	    }
	    async next() {
	        if (this.trav >= this.items.length) {
	            return { value: null, done: true };
	        }
	        const item = this.items[this.trav];
	        this.trav++;
	        return { value: deepClone(item), done: false };
	    }
	}
	class FunctionCallIterator extends LazyIterator {
	    constructor(nextFn) {
	        super();
	        this.nextFn = nextFn;
	    }
	    summary() {
	        return `Function call`;
	    }
	    async next() {
	        try {
	            return this.nextFn();
	        }
	        catch (e) {
	            // Modify the error message but leave the stack trace intact
	            e.message =
	                `Error thrown while iterating through a dataset: ${e.message}`;
	            throw e;
	        }
	    }
	}
	class SerialIterator extends LazyIterator {
	    constructor(upstream) {
	        super();
	        this.upstream = upstream;
	        this.lastRead = Promise.resolve({ value: null, done: false });
	    }
	    summary() {
	        return `${this.upstream.summary()} -> Serial`;
	    }
	    async next() {
	        // This sets this.lastRead to a new Promise right away, as opposed to
	        // saying `await this.lastRead; this.lastRead = this.serialNext();` which
	        // would not work because this.nextRead would be updated only after the
	        // promise resolves.
	        this.lastRead = this.lastRead.then(() => this.serialNext());
	        return this.lastRead;
	    }
	    async serialNext() {
	        return this.upstream.next();
	    }
	}
	class SkipIterator extends LazyIterator {
	    constructor(upstream, maxCount) {
	        super();
	        this.upstream = upstream;
	        this.maxCount = maxCount;
	        // Local state that should not be clobbered by out-of-order execution.
	        this.count = 0;
	        this.lastRead = Promise.resolve({ value: null, done: false });
	    }
	    summary() {
	        return `${this.upstream.summary()} -> Skip`;
	    }
	    async next() {
	        // This sets this.lastRead to a new Promise right away, as opposed to
	        // saying `await this.lastRead; this.lastRead = this.serialNext();` which
	        // would not work because this.nextRead would be updated only after the
	        // promise resolves.
	        this.lastRead = this.lastRead.then(() => this.serialNext());
	        return this.lastRead;
	    }
	    async serialNext() {
	        // TODO(soergel): consider tradeoffs of reading in parallel, eg.
	        // collecting next() promises in an Array and then waiting for
	        // Promise.all() of those. Benefit: pseudo-parallel execution.  Drawback:
	        // maybe delayed GC.
	        while (this.count++ < this.maxCount) {
	            const skipped = await this.upstream.next();
	            // short-circuit if upstream is already empty
	            if (skipped.done) {
	                return skipped;
	            }
	            tf__namespace.dispose(skipped.value);
	        }
	        return this.upstream.next();
	    }
	}
	class TakeIterator extends LazyIterator {
	    constructor(upstream, maxCount) {
	        super();
	        this.upstream = upstream;
	        this.maxCount = maxCount;
	        this.count = 0;
	    }
	    summary() {
	        return `${this.upstream.summary()} -> Take`;
	    }
	    async next() {
	        if (this.count++ >= this.maxCount) {
	            return { value: null, done: true };
	        }
	        return this.upstream.next();
	    }
	}
	// Note this batch just groups items into row-wise element arrays.
	// Rotating these to a column-wise representation happens only at the dataset
	// level.
	class RowMajorBatchIterator extends LazyIterator {
	    constructor(upstream, batchSize, enableSmallLastBatch = true) {
	        super();
	        this.upstream = upstream;
	        this.batchSize = batchSize;
	        this.enableSmallLastBatch = enableSmallLastBatch;
	        this.lastRead = Promise.resolve({ value: null, done: false });
	    }
	    summary() {
	        return `${this.upstream.summary()} -> RowMajorBatch`;
	    }
	    async next() {
	        // This sets this.lastRead to a new Promise right away, as opposed to
	        // saying `await this.lastRead; this.lastRead = this.serialNext();` which
	        // would not work because this.nextRead would be updated only after the
	        // promise resolves.
	        this.lastRead = this.lastRead.then(() => this.serialNext());
	        return this.lastRead;
	    }
	    async serialNext() {
	        const batch = [];
	        while (batch.length < this.batchSize) {
	            const item = await this.upstream.next();
	            if (item.done) {
	                if (this.enableSmallLastBatch && batch.length > 0) {
	                    return { value: batch, done: false };
	                }
	                return { value: null, done: true };
	            }
	            batch.push(item.value);
	        }
	        return { value: batch, done: false };
	    }
	}
	class FilterIterator extends LazyIterator {
	    constructor(upstream, predicate) {
	        super();
	        this.upstream = upstream;
	        this.predicate = predicate;
	        this.lastRead = Promise.resolve({ value: null, done: false });
	    }
	    summary() {
	        return `${this.upstream.summary()} -> Filter`;
	    }
	    async next() {
	        // This sets this.lastRead to a new Promise right away, as opposed to
	        // saying `await this.lastRead; this.lastRead = this.serialNext();` which
	        // would not work because this.nextRead would be updated only after the
	        // promise resolves.
	        this.lastRead = this.lastRead.then(() => this.serialNext());
	        return this.lastRead;
	    }
	    async serialNext() {
	        while (true) {
	            const item = await this.upstream.next();
	            if (item.done || this.predicate(item.value)) {
	                return item;
	            }
	            tf__namespace.dispose(item.value);
	        }
	    }
	}
	class MapIterator extends LazyIterator {
	    constructor(upstream, transform) {
	        super();
	        this.upstream = upstream;
	        this.transform = transform;
	    }
	    summary() {
	        return `${this.upstream.summary()} -> Map`;
	    }
	    async next() {
	        const item = await this.upstream.next();
	        if (item.done) {
	            return { value: null, done: true };
	        }
	        const inputTensors = tf__namespace.tensor_util.getTensorsInContainer(item.value);
	        // Careful: the transform may mutate the item in place.
	        // That's why we have to remember the input Tensors above, and then
	        // below dispose only those that were not passed through to the output.
	        // Note too that the transform function is responsible for tidying
	        // any intermediate Tensors.  Here we are concerned only about the
	        // inputs.
	        const mapped = this.transform(item.value);
	        const outputTensors = tf__namespace.tensor_util.getTensorsInContainer(mapped);
	        // TODO(soergel) faster intersection
	        // TODO(soergel) move to tf.disposeExcept(in, out)?
	        for (const t of inputTensors) {
	            if (!tf__namespace.tensor_util.isTensorInList(t, outputTensors)) {
	                t.dispose();
	            }
	        }
	        return { value: mapped, done: false };
	    }
	}
	class ErrorHandlingLazyIterator extends LazyIterator {
	    constructor(upstream, handler) {
	        super();
	        this.upstream = upstream;
	        this.handler = handler;
	        this.count = 0;
	        this.lastRead = Promise.resolve({ value: null, done: false });
	    }
	    summary() {
	        return `${this.upstream.summary()} -> handleErrors`;
	    }
	    async next() {
	        // This sets this.lastRead to a new Promise right away, as opposed to
	        // saying `await this.lastRead; this.lastRead = this.serialNext();` which
	        // would not work because this.nextRead would be updated only after the
	        // promise resolves.
	        this.lastRead = this.lastRead.then(() => this.serialNext());
	        return this.lastRead;
	    }
	    async serialNext() {
	        while (true) {
	            try {
	                return await this.upstream.next();
	            }
	            catch (e) {
	                if (!this.handler(e)) {
	                    return { value: null, done: true };
	                }
	                // If the handler returns true, loop and fetch the next upstream item.
	                // If the upstream iterator throws an endless stream of errors, and if
	                // the handler says to ignore them, then we loop forever here.  That is
	                // the correct behavior-- it's up to the handler to decide when to stop.
	            }
	        }
	    }
	}
	class AsyncMapIterator extends LazyIterator {
	    constructor(upstream, transform) {
	        super();
	        this.upstream = upstream;
	        this.transform = transform;
	    }
	    summary() {
	        return `${this.upstream.summary()} -> AsyncMap`;
	    }
	    async next() {
	        const item = await this.upstream.next();
	        if (item.done) {
	            return { value: null, done: true };
	        }
	        const inputTensors = tf__namespace.tensor_util.getTensorsInContainer(item.value);
	        // Careful: the transform may mutate the item in place.
	        // That's why we have to remember the input Tensors above, and then
	        // below dispose only those that were not passed through to the output.
	        // Note too that the transform function is responsible for tidying
	        // any intermediate Tensors.  Here we are concerned only about the
	        // inputs.
	        const mapped = await this.transform(item.value);
	        const outputTensors = tf__namespace.tensor_util.getTensorsInContainer(mapped);
	        // TODO(soergel) faster intersection
	        // TODO(soergel) move to tf.disposeExcept(in, out)?
	        for (const t of inputTensors) {
	            if (!tf__namespace.tensor_util.isTensorInList(t, outputTensors)) {
	                t.dispose();
	            }
	        }
	        return { value: mapped, done: false };
	    }
	}
	// Iterators that maintain a queue of pending items
	// ============================================================================
	/**
	 * A base class for transforming streams that operate by maintaining an
	 * output queue of elements that are ready to return via next().  This is
	 * commonly required when the transformation is 1-to-many:  A call to next()
	 * may trigger a call to the underlying stream, which will produce many
	 * mapped elements of this stream-- of which we need to return only one, so
	 * we have to queue the rest.
	 */
	class OneToManyIterator extends LazyIterator {
	    constructor() {
	        super();
	        this.outputQueue = new GrowingRingBuffer();
	        this.lastRead = Promise.resolve({ value: null, done: false });
	    }
	    async next() {
	        // This sets this.lastRead to a new Promise right away, as opposed to
	        // saying `await this.lastRead; this.lastRead = this.serialNext();` which
	        // would not work because this.nextRead would be updated only after the
	        // promise resolves.
	        this.lastRead = this.lastRead.then(() => this.serialNext());
	        return this.lastRead;
	    }
	    async serialNext() {
	        // Fetch so that the queue contains at least one item if possible.
	        // If the upstream source is exhausted, AND there are no items left in
	        // the output queue, then this stream is also exhausted.
	        while (this.outputQueue.length() === 0) {
	            // TODO(soergel): consider parallel reads.
	            if (!await this.pump()) {
	                return { value: null, done: true };
	            }
	        }
	        return { value: this.outputQueue.shift(), done: false };
	    }
	}
	class FlatmapIterator extends OneToManyIterator {
	    constructor(upstream, transform) {
	        super();
	        this.upstream = upstream;
	        this.transform = transform;
	    }
	    summary() {
	        return `${this.upstream.summary()} -> Flatmap`;
	    }
	    async pump() {
	        const item = await this.upstream.next();
	        if (item.done) {
	            return false;
	        }
	        const inputTensors = tf__namespace.tensor_util.getTensorsInContainer(item.value);
	        // Careful: the transform may mutate the item in place.
	        // that's why we have to remember the input Tensors above, and then
	        // below dispose only those that were not passed through to the output.
	        // Note too that the transform function is responsible for tidying any
	        // intermediate Tensors.  Here we are concerned only about the inputs.
	        const mappedArray = this.transform(item.value);
	        const outputTensors = tf__namespace.tensor_util.getTensorsInContainer(mappedArray);
	        this.outputQueue.pushAll(mappedArray);
	        // TODO(soergel) faster intersection, and deduplicate outputTensors
	        // TODO(soergel) move to tf.disposeExcept(in, out)?
	        for (const t of inputTensors) {
	            if (!tf__namespace.tensor_util.isTensorInList(t, outputTensors)) {
	                t.dispose();
	            }
	        }
	        return true;
	    }
	}
	/**
	 * Provides a `LazyIterator` that concatenates a stream of underlying
	 * streams.
	 *
	 * Doing this in a concurrency-safe way requires some trickery.  In
	 * particular, we want this stream to return the elements from the
	 * underlying streams in the correct order according to when next() was
	 * called, even if the resulting Promises resolve in a different order.
	 */
	class ChainedIterator extends LazyIterator {
	    constructor(iterators, baseErrorHandler) {
	        super();
	        this.baseErrorHandler = baseErrorHandler;
	        // Strict Promise execution order:
	        // a next() call may not even begin until the previous one completes.
	        this.lastRead = null;
	        // Local state that should not be clobbered by out-of-order execution.
	        this.iterator = null;
	        this.moreIterators = iterators;
	    }
	    summary() {
	        const upstreamSummaries = 'TODO: fill in upstream of chained summaries';
	        return `${upstreamSummaries} -> Chained`;
	    }
	    async next() {
	        this.lastRead = this.readFromChain(this.lastRead);
	        return this.lastRead;
	    }
	    async readFromChain(lastRead) {
	        // Must await on the previous read since the previous read may have advanced
	        // the stream of streams, from which we need to read.
	        // This is unfortunate since we can't parallelize reads. Which means
	        // prefetching of chained streams is a no-op.
	        // One solution is to prefetch immediately upstream of this.
	        await lastRead;
	        if (this.iterator == null) {
	            const iteratorResult = await this.moreIterators.next();
	            if (iteratorResult.done) {
	                // No more streams to stream from.
	                return { value: null, done: true };
	            }
	            this.iterator = iteratorResult.value;
	            if (this.baseErrorHandler != null) {
	                this.iterator = this.iterator.handleErrors(this.baseErrorHandler);
	            }
	        }
	        const itemResult = await this.iterator.next();
	        if (itemResult.done) {
	            this.iterator = null;
	            return this.readFromChain(lastRead);
	        }
	        return itemResult;
	    }
	}
	var ZipMismatchMode;
	(function (ZipMismatchMode) {
	    ZipMismatchMode[ZipMismatchMode["FAIL"] = 0] = "FAIL";
	    ZipMismatchMode[ZipMismatchMode["SHORTEST"] = 1] = "SHORTEST";
	    ZipMismatchMode[ZipMismatchMode["LONGEST"] = 2] = "LONGEST"; // use nulls for exhausted streams; use up the longest stream.
	})(ZipMismatchMode || (ZipMismatchMode = {}));
	/**
	 * Provides a `LazyIterator` that zips together an array, dict, or nested
	 * structure of `LazyIterator`s (and perhaps additional constants).
	 *
	 * The underlying streams must provide elements in a consistent order such
	 * that they correspond.
	 *
	 * Typically, the underlying streams should have the same number of
	 * elements. If they do not, the behavior is determined by the
	 * `mismatchMode` argument.
	 *
	 * The nested structure of the `iterators` argument determines the
	 * structure of elements in the resulting iterator.
	 *
	 * Doing this in a concurrency-safe way requires some trickery.  In
	 * particular, we want this stream to return the elements from the
	 * underlying streams in the correct order according to when next() was
	 * called, even if the resulting Promises resolve in a different order.
	 *
	 * @param iterators: An array or object containing LazyIterators at the
	 * leaves.
	 * @param mismatchMode: Determines what to do when one underlying iterator
	 * is exhausted before the others.  `ZipMismatchMode.FAIL` (the default)
	 * causes an error to be thrown in this case.  `ZipMismatchMode.SHORTEST`
	 * causes the zipped iterator to terminate with the furst underlying
	 * streams, so elements remaining on the longer streams are ignored.
	 * `ZipMismatchMode.LONGEST` causes the zipped stream to continue, filling
	 * in nulls for the exhausted streams, until all streams are exhausted.
	 */
	class ZipIterator extends LazyIterator {
	    constructor(iterators, mismatchMode = ZipMismatchMode.FAIL) {
	        super();
	        this.iterators = iterators;
	        this.mismatchMode = mismatchMode;
	        this.count = 0;
	        this.currentPromise = null;
	    }
	    summary() {
	        const upstreamSummaries = 'TODO: fill in upstream of zip summaries';
	        return `{${upstreamSummaries}} -> Zip`;
	    }
	    async nextState(afterState) {
	        // This chaining ensures that the underlying next() are not even called
	        // before the previous ones have resolved.
	        await afterState;
	        // Collect underlying iterator "done" signals as a side effect in
	        // getNext()
	        let numIterators = 0;
	        let iteratorsDone = 0;
	        function getNext(container) {
	            if (container instanceof LazyIterator) {
	                const result = container.next();
	                return {
	                    value: result.then(x => {
	                        numIterators++;
	                        if (x.done) {
	                            iteratorsDone++;
	                        }
	                        return x.value;
	                    }),
	                    recurse: false
	                };
	            }
	            else {
	                return { value: null, recurse: true };
	            }
	        }
	        const mapped = await deepMapAndAwaitAll(this.iterators, getNext);
	        if (numIterators === iteratorsDone) {
	            // The streams have all ended.
	            return { value: null, done: true };
	        }
	        if (iteratorsDone > 0) {
	            switch (this.mismatchMode) {
	                case ZipMismatchMode.FAIL:
	                    throw new Error('Zipped streams should have the same length. ' +
	                        `Mismatched at element ${this.count}.`);
	                case ZipMismatchMode.SHORTEST:
	                    return { value: null, done: true };
	                case ZipMismatchMode.LONGEST:
	                // Continue.  The exhausted streams already produced value: null.
	            }
	        }
	        this.count++;
	        return { value: mapped, done: false };
	    }
	    async next() {
	        this.currentPromise = this.nextState(this.currentPromise);
	        return this.currentPromise;
	    }
	}
	// Iterators that maintain a ring buffer of pending promises
	// ============================================================================
	/**
	 * A stream that prefetches a given number of items from an upstream source,
	 * returning them in FIFO order.
	 *
	 * Note this prefetches Promises, but makes no guarantees about when those
	 * Promises resolve.
	 */
	class PrefetchIterator extends LazyIterator {
	    constructor(upstream, bufferSize) {
	        super();
	        this.upstream = upstream;
	        this.bufferSize = bufferSize;
	        this.buffer = new RingBuffer(bufferSize);
	    }
	    summary() {
	        return `${this.upstream.summary()} -> Prefetch`;
	    }
	    /**
	     * Refill the prefetch buffer.  Returns only after the buffer is full, or
	     * the upstream source is exhausted.
	     */
	    refill() {
	        while (!this.buffer.isFull()) {
	            const v = this.upstream.next();
	            this.buffer.push(v);
	        }
	    }
	    next() {
	        this.refill();
	        // This shift will never throw an error because the buffer is always
	        // full after a refill. If the stream is exhausted, the buffer will be
	        // full of Promises that will resolve to the end-of-stream signal.
	        return this.buffer.shift();
	    }
	}
	/**
	 * A stream that performs a sliding-window random shuffle on an upstream
	 * source. This is like a `PrefetchIterator` except that the items are
	 * returned in randomized order.  Mixing naturally improves as the buffer
	 * size increases.
	 */
	class ShuffleIterator extends PrefetchIterator {
	    constructor(upstream, windowSize, seed) {
	        super(upstream, windowSize);
	        this.upstream = upstream;
	        this.windowSize = windowSize;
	        // Local state that should not be clobbered by out-of-order execution.
	        this.upstreamExhausted = false;
	        this.random = seedrandom.alea(seed || tf__namespace.util.now().toString());
	        this.lastRead = Promise.resolve({ value: null, done: false });
	    }
	    async next() {
	        // This sets this.lastRead to a new Promise right away, as opposed to
	        // saying `await this.lastRead; this.lastRead = this.serialNext();` which
	        // would not work because this.nextRead would be updated only after the
	        // promise resolves.
	        this.lastRead = this.lastRead.then(() => this.serialNext());
	        return this.lastRead;
	    }
	    randomInt(max) {
	        return Math.floor(this.random() * max);
	    }
	    chooseIndex() {
	        return this.randomInt(this.buffer.length());
	    }
	    async serialNext() {
	        // TODO(soergel): consider performance
	        if (!this.upstreamExhausted) {
	            this.refill();
	        }
	        while (!this.buffer.isEmpty()) {
	            const chosenIndex = this.chooseIndex();
	            const result = await this.buffer.shuffleExcise(chosenIndex);
	            if (result.done) {
	                this.upstreamExhausted = true;
	            }
	            else {
	                this.refill();
	                return result;
	            }
	        }
	        return { value: null, done: true };
	    }
	}

	/**
	 * @license
	 * Copyright 2018 Google LLC. All Rights Reserved.
	 * Licensed under the Apache License, Version 2.0 (the "License");
	 * you may not use this file except in compliance with the License.
	 * You may obtain a copy of the License at
	 *
	 * http://www.apache.org/licenses/LICENSE-2.0
	 *
	 * Unless required by applicable law or agreed to in writing, software
	 * distributed under the License is distributed on an "AS IS" BASIS,
	 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
	 * See the License for the specific language governing permissions and
	 * limitations under the License.
	 *
	 * =============================================================================
	 */
	// TODO(soergel): consider vectorized operations within the pipeline.
	/**
	 * Represents a potentially large list of independent data elements (typically
	 * 'samples' or 'examples').
	 *
	 * A 'data example' may be a primitive, an array, a map from string keys to
	 * values, or any nested structure of these.
	 *
	 * A `Dataset` represents an ordered collection of elements, together with a
	 * chain of transformations to be performed on those elements. Each
	 * transformation is a method of `Dataset` that returns another `Dataset`, so
	 * these may be chained, e.g.
	 * `const processedDataset = rawDataset.filter(...).map(...).batch(...)`.
	 *
	 * Data loading and transformation is done in a lazy, streaming fashion.  The
	 * dataset may be iterated over multiple times; each iteration starts the data
	 * loading anew and recapitulates the transformations.
	 *
	 * A `Dataset` is typically processed as a stream of unbatched examples -- i.e.,
	 * its transformations are applied one example at a time. Batching produces a
	 * new `Dataset` where each element is a batch. Batching should usually come
	 * last in a pipeline, because data transformations are easier to express on a
	 * per-example basis than on a per-batch basis.
	 *
	 * The following code examples are calling `await dataset.forEachAsync(...)` to
	 * iterate once over the entire dataset in order to print out the data.
	 *
	 * @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'}
	 */
	class Dataset {
	    constructor() {
	        this.size = null;
	    }
	    // TODO(soergel): Make Datasets report whether repeated iterator() calls
	    // produce the same result (e.g., reading from a file) or different results
	    // (e.g., from the webcam).  Currently we don't make this distinction but it
	    // could be important for the user to know.
	    // abstract isDeterministic(): boolean;
	    /**
	     * Groups elements into batches.
	     *
	     * It is assumed that each of the incoming dataset elements has the same
	     * structure -- i.e. the same set of keys at each location in an object
	     * hierarchy.  For each key, the resulting `Dataset` provides a batched
	     * element collecting all of the incoming values for that key.
	     *
	     *  * Incoming primitives are grouped into a 1-D Tensor.
	     *  * Incoming Tensors are grouped into a new Tensor where the 0th axis is
	     *    the batch dimension.
	     *  * Incoming arrays are converted to Tensor and then batched.
	     *  * A nested array is interpreted as an n-D Tensor, so the batched result
	     *    has n+1 dimensions.
	     *  * An array that cannot be converted to Tensor produces an error.
	     *
	     * If an array should not be batched as a unit, it should first be converted
	     * to an object with integer keys.
	     *
	     * Here are a few examples:
	     *
	     * Batch a dataset of numbers:
	     * ```js
	     * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8]).batch(4);
	     * await a.forEachAsync(e => e.print());
	     * ```
	     *
	     * Batch a dataset of arrays:
	     * ```js
	     * const b = tf.data.array([[1], [2], [3], [4], [5], [6], [7], [8]]).batch(4);
	     * await b.forEachAsync(e => e.print());
	     * ```
	     *
	     * Batch a dataset of objects:
	     * ```js
	     * const c = tf.data.array([{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13},
	     *   {a: 4, b: 14}, {a: 5, b: 15}, {a: 6, b: 16}, {a: 7, b: 17},
	     *   {a: 8, b: 18}]).batch(4);
	     * await c.forEachAsync(e => {
	     *   console.log('{');
	     *   for(var key in e) {
	     *     console.log(key+':');
	     *     e[key].print();
	     *   }
	     *   console.log('}');
	     * })
	     * ```
	     *
	     * @param batchSize The number of elements desired per batch.
	     * @param smallLastBatch Whether to emit the final batch when it has fewer
	     *   than batchSize elements. Default true.
	     * @returns A `Dataset`, from which a stream of batches can be obtained.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    batch(batchSize, smallLastBatch = true) {
	        const base = this;
	        tf__namespace.util.assert(batchSize > 0, () => `batchSize needs to be positive, but it is
      ${batchSize}`);
	        let size;
	        if (this.size === Infinity || this.size == null) {
	            // If the size of this dataset is infinity or null, the new size keeps the
	            // same.
	            size = this.size;
	        }
	        else if (smallLastBatch) {
	            // If the size of this dataset is known and include small last batch, the
	            // new size is full batch count plus last batch.
	            size = Math.ceil(this.size / batchSize);
	        }
	        else {
	            // If the size of this dataset is known and not include small last batch,
	            // the new size is full batch count.
	            size = Math.floor(this.size / batchSize);
	        }
	        return datasetFromIteratorFn(async () => {
	            return (await base.iterator())
	                .columnMajorBatch(batchSize, smallLastBatch, deepBatchConcat);
	        }, size);
	    }
	    /**
	     * Concatenates this `Dataset` with another.
	     *
	     * ```js
	     * const a = tf.data.array([1, 2, 3]);
	     * const b = tf.data.array([4, 5, 6]);
	     * const c = a.concatenate(b);
	     * await c.forEachAsync(e => console.log(e));
	     * ```
	     *
	     * @param dataset A `Dataset` to be concatenated onto this one.
	     * @returns A `Dataset`.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    concatenate(dataset) {
	        const base = this;
	        let size;
	        if (this.size === Infinity || dataset.size === Infinity) {
	            // If the size of any of these two dataset is infinity, new size is
	            // infinity.
	            size = Infinity;
	        }
	        else if (this.size != null && dataset.size != null) {
	            // If the size of both datasets are known and not infinity, new size is
	            // sum the size of these two datasets.
	            size = this.size + dataset.size;
	        }
	        else {
	            // If neither of these two datasets has infinite size and any of these two
	            // datasets' size is null, the new size is null.
	            size = null;
	        }
	        return datasetFromIteratorFn(async () => (await base.iterator()).concatenate(await dataset.iterator()), size);
	    }
	    /**
	     * Filters this dataset according to `predicate`.
	     *
	     * ```js
	     * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
	     *   .filter(x => x%2 === 0);
	     * await a.forEachAsync(e => console.log(e));
	     * ```
	     *
	     * @param predicate A function mapping a dataset element to a boolean or a
	     * `Promise` for one.
	     *
	     * @returns A `Dataset` of elements for which the predicate was true.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    filter(predicate) {
	        const base = this;
	        let size;
	        if (this.size === Infinity) {
	            // If the size of this dataset is infinity, new size is infinity
	            size = Infinity;
	        }
	        else {
	            // If this dataset has limited elements, new size is null because it might
	            // exhausted randomly.
	            size = null;
	        }
	        return datasetFromIteratorFn(async () => {
	            return (await base.iterator()).filter(x => tf__namespace.tidy(() => predicate(x)));
	        }, size);
	    }
	    /**
	     * Apply a function to every element of the dataset.
	     *
	     * After the function is applied to a dataset element, any Tensors contained
	     * within that element are disposed.
	     *
	     * ```js
	     * const a = tf.data.array([1, 2, 3]);
	     * await a.forEachAsync(e => console.log(e));
	     * ```
	     *
	     * @param f A function to apply to each dataset element.
	     * @returns A `Promise` that resolves after all elements have been processed.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    async forEachAsync(f) {
	        return (await this.iterator()).forEachAsync(f);
	    }
	    /**
	     * Maps this dataset through a 1-to-1 transform.
	     *
	     * ```js
	     * const a = tf.data.array([1, 2, 3]).map(x => x*x);
	     * await a.forEachAsync(e => console.log(e));
	     * ```
	     *
	     * @param transform A function mapping a dataset element to a transformed
	     *   dataset element.
	     *
	     * @returns A `Dataset` of transformed elements.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    map(transform) {
	        const base = this;
	        return datasetFromIteratorFn(async () => {
	            return (await base.iterator()).map(x => tf__namespace.tidy(() => transform(x)));
	        }, this.size);
	    }
	    /**
	     * Maps this dataset through an async 1-to-1 transform.
	     *
	     * ```js
	     * const a =
	     *  tf.data.array([1, 2, 3]).mapAsync(x => new Promise(function(resolve){
	     *    setTimeout(() => {
	     *      resolve(x * x);
	     *    }, Math.random()*1000 + 500);
	     *  }));
	     * console.log(await a.toArray());
	     * ```
	     *
	     * @param transform A function mapping a dataset element to a `Promise` for a
	     *   transformed dataset element.  This transform is responsible for disposing
	     *   any intermediate `Tensor`s, i.e. by wrapping its computation in
	     *   `tf.tidy()`; that cannot be automated here (as it is in the synchronous
	     *   `map()` case).
	     *
	     * @returns A `Dataset` of transformed elements.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    mapAsync(transform) {
	        const base = this;
	        return datasetFromIteratorFn(async () => {
	            return (await base.iterator()).mapAsync(transform);
	        }, this.size);
	    }
	    /**
	     *  Creates a `Dataset` that prefetches elements from this dataset.
	     *
	     * @param bufferSize: An integer specifying the number of elements to be
	     *   prefetched.
	     * @returns A `Dataset`.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    prefetch(bufferSize) {
	        if (bufferSize == null) {
	            throw new RangeError('`Dataset.prefetch()` requires bufferSize to be specified.');
	        }
	        const base = this;
	        return datasetFromIteratorFn(async () => (await base.iterator()).prefetch(bufferSize), this.size);
	    }
	    /**
	     * Repeats this dataset `count` times.
	     *
	     * NOTE: If this dataset is a function of global state (e.g. a random number
	     * generator), then different repetitions may produce different elements.
	     *
	     * ```js
	     * const a = tf.data.array([1, 2, 3]).repeat(3);
	     * await a.forEachAsync(e => console.log(e));
	     * ```
	     *
	     * @param count: (Optional) An integer, representing the number of times
	     *   the dataset should be repeated. The default behavior (if `count` is
	     *   `undefined` or negative) is for the dataset be repeated indefinitely.
	     * @returns A `Dataset`.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    repeat(count) {
	        const base = this;
	        let size;
	        if (this.size != null && count > 0) {
	            // If this dataset has size and count is positive, new size is current
	            // size multiply count. This also covers the case that current size is
	            // infinity.
	            size = this.size * count;
	        }
	        else if (count === 0) {
	            // If count is 0, new size is 0.
	            size = 0;
	        }
	        else if (this.size != null && (count === undefined || count < 0)) {
	            // If this dataset has size and count is undefined or negative, the
	            // dataset will be repeated indefinitely and new size is infinity.
	            size = Infinity;
	        }
	        else {
	            // If the size of this dataset is null, the new dataset's size is null.
	            size = null;
	        }
	        return datasetFromIteratorFn(async () => {
	            const iteratorIterator = iteratorFromFunction(async () => ({ value: await base.iterator(), done: false }));
	            return iteratorFromConcatenated(iteratorIterator.take(count));
	        }, size);
	    }
	    /**
	     * Creates a `Dataset` that skips `count` initial elements from this dataset.
	     *
	     * ```js
	     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).skip(3);
	     * await a.forEachAsync(e => console.log(e));
	     * ```
	     *
	     * @param count: The number of elements of this dataset that should be skipped
	     *   to form the new dataset.  If `count` is greater than the size of this
	     *   dataset, the new dataset will contain no elements.  If `count`
	     *   is `undefined` or negative, skips the entire dataset.
	     *
	     * @returns A `Dataset`.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    skip(count) {
	        const base = this;
	        let size;
	        if (this.size != null && count >= 0 && this.size >= count) {
	            // If the size of this dataset is greater than count, the new dataset's
	            // size is current size minus skipped size.This also covers the case that
	            // current size is infinity.
	            size = this.size - count;
	        }
	        else if (this.size != null &&
	            (this.size < count || count === undefined || count < 0)) {
	            // If the size of this dataset is smaller than count, or count is
	            // undefined or negative, skips the entire dataset and the new size is 0.
	            size = 0;
	        }
	        else {
	            // If the size of this dataset is null, the new dataset's size is null.
	            size = null;
	        }
	        return datasetFromIteratorFn(async () => (await base.iterator()).skip(count), size);
	    }
	    /**
	     * Pseudorandomly shuffles the elements of this dataset. This is done in a
	     * streaming manner, by sampling from a given number of prefetched elements.
	     *
	     * ```js
	     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).shuffle(3);
	     * await a.forEachAsync(e => console.log(e));
	     * ```
	     *
	     * @param bufferSize: An integer specifying the number of elements from this
	     *   dataset from which the new dataset will sample.
	     * @param seed: (Optional) An integer specifying the random seed that will
	     *   be used to create the distribution.
	     * @param reshuffleEachIteration: (Optional) A boolean, which if true
	     *   indicates that the dataset should be pseudorandomly reshuffled each time
	     *   it is iterated over. If false, elements will be returned in the same
	     *   shuffled order on each iteration. (Defaults to `true`.)
	     * @returns A `Dataset`.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    shuffle(bufferSize, seed, reshuffleEachIteration = true) {
	        if (bufferSize == null || bufferSize < 0) {
	            if (this.size == null) {
	                throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.');
	            }
	            else {
	                throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.  ' +
	                    'If your data fits in main memory (for regular JS objects), ' +
	                    'and/or GPU memory (for `tf.Tensor`s), consider setting ' +
	                    `bufferSize to the dataset size (${this.size} elements)`);
	            }
	        }
	        const base = this;
	        const random = seedrandom.alea(seed || tf__namespace.util.now().toString());
	        return datasetFromIteratorFn(async () => {
	            let seed2 = random.int32();
	            if (reshuffleEachIteration) {
	                seed2 += random.int32();
	            }
	            return (await base.iterator()).shuffle(bufferSize, seed2.toString());
	        }, this.size);
	    }
	    /**
	     * Creates a `Dataset` with at most `count` initial elements from this
	     * dataset.
	     *
	     * ```js
	     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).take(3);
	     * await a.forEachAsync(e => console.log(e));
	     * ```
	     *
	     * @param count: The number of elements of this dataset that should be taken
	     *   to form the new dataset.  If `count` is `undefined` or negative, or if
	     *   `count` is greater than the size of this dataset, the new dataset will
	     *   contain all elements of this dataset.
	     * @returns A `Dataset`.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    take(count) {
	        const base = this;
	        let size;
	        if (this.size != null && this.size > count) {
	            // If the size of this dataset is greater than count, the new dataset's
	            // size is count.
	            size = count;
	        }
	        else if (this.size != null && this.size <= count) {
	            // If the size of this dataset is equal or smaller than count, the new
	            // dataset's size is the size of this dataset.
	            size = this.size;
	        }
	        else {
	            // If the size of this dataset is null, the new dataset's size is null.
	            size = null;
	        }
	        return datasetFromIteratorFn(async () => (await base.iterator()).take(count), size);
	    }
	    /**
	     * Collect all elements of this dataset into an array.
	     *
	     * Obviously this will succeed only for small datasets that fit in memory.
	     * Useful for testing and generally should be avoided if possible.
	     *
	     * ```js
	     * const a = tf.data.array([1, 2, 3, 4, 5, 6]);
	     * console.log(await a.toArray());
	     * ```
	     *
	     * @returns A Promise for an array of elements, which will resolve
	     *   when a new stream has been obtained and fully consumed.
	     *
	     * @doc {heading: 'Data', subheading: 'Classes'}
	     */
	    async toArray() {
	        if (this.size === Infinity) {
	            throw new Error('Can not convert infinite data stream to array.');
	        }
	        return (await this.iterator()).toArray();
	    }
	    /**
	     * Collect all elements of this dataset into an array with prefetching 100
	     * elements. This is useful for testing, because the prefetch changes the
	     * order in which the Promises are resolved along the processing pipeline.
	     * This may help expose bugs where results are dependent on the order of
	     * Promise resolution rather than on the logical order of the stream (i.e.,
	     * due to hidden mutable state).
	     *
	     * @returns A Promise for an array of elements, which will resolve
	     *   when a new stream has been obtained and fully consumed.
	     */
	    async toArrayForTest() {
	        if (this.size === Infinity) {
	            throw new Error('Can not convert infinite data stream to array.');
	        }
	        return (await this.iterator()).toArrayForTest();
	    }
	}
	// TODO(soergel): deep sharded shuffle, where supported
	Dataset.MAX_BUFFER_SIZE = 10000;
	/**
	 * Create a `Dataset` defined by a provided iterator() function.
	 *
	 * ```js
	 * let i = -1;
	 * const func = () =>
	 *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};
	 * const iter = tf.data.iteratorFromFunction(func);
	 * const ds = tf.data.datasetFromIteratorFn(iter);
	 * await ds.forEachAsync(e => console.log(e));
	 * ```
	 */
	function datasetFromIteratorFn(iteratorFn, size = null) {
	    return new class extends Dataset {
	        constructor() {
	            super(...arguments);
	            this.size = size;
	        }
	        /*
	         * Provid